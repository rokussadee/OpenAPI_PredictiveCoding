{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# metacognitive prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import sys\n",
    "import queue\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(os.getenv(\"SYS_PATH\"))\n",
    "print(f'{os.getenv(\"SYS_PATH\")}')\n",
    "print(sys.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "from openai import OpenAI, OpenAIError\n",
    "from ipywidgets import widgets\n",
    "from IPython.display import display, clear_output\n",
    "from bson.objectid import ObjectId\n",
    "from typing import Optional, List, Dict\n",
    "import threading\n",
    "\n",
    "from models.user import create_user\n",
    "from models.chat_message import create_chat_message, ChatMessageData\n",
    "from models.conversation import create_conversation\n",
    "from models.chatbot_response import create_chatbot_response\n",
    "from models.expectation import create_expectation\n",
    "from models.expectation_revision import create_expectation_revision\n",
    "from models.violation import create_violation\n",
    "\n",
    "from utils.model_operations import create_model, get_model, get_chat_history\n",
    "from services.mock_service import mock_user_data, mock_expectation_data, mock_expectation_revision_data, mock_message_data, mock_violation_data, mock_chatbot_response_data\n",
    "from services.prompt_builder_service import build_context_prompt, build_expectation_prompt, build_expectation_revision_prompt, build_voe_prompt\n",
    "from utils.helpers import CustomOutput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../services/mongo_service.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from services.mongo_service import mongo_client, ping_client\n",
    "db_client = mongo_client()\n",
    "ping_client(db_client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_input = \"\"\n",
    "chat_output = CustomOutput()\n",
    "user_input_queue = queue.Queue()\n",
    "expectations_queue = queue.Queue()\n",
    "\n",
    "saved_user_id = None\n",
    "saved_conversation_id = None\n",
    "set_user = None\n",
    "set_conversation = None\n",
    "chat_history = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_items():\n",
    "    global saved_user_id, saved_conversation_id\n",
    "    chat_output.append_stdout(f\"\"\"Here follow some examples of:\n",
    "    - user\n",
    "    - conversation\n",
    "    - expectation\n",
    "    - expectation revision\n",
    "    - user chat message\n",
    "    - violation (VoE thought & context)\n",
    "    - chatbot response message\"\"\")\n",
    "    test_user = create_user(email=mock_user_data[\"email\"], password=mock_user_data[\"password\"])\n",
    "    saved_user_id = create_model(collection_name=\"users\", model_data=test_user, client=db_client)\n",
    "    chat_output.append_stdout(f\"test_user: {test_user} with id: {saved_user_id}\\nis of type{type(test_user)}\\n\\n\", stream=True)\n",
    "    \n",
    "    test_conversation = create_conversation(user_id=saved_user_id)\n",
    "    saved_conversation_id = create_model(collection_name=\"conversations\", model_data=test_conversation, client=db_client)\n",
    "    chat_output.append_stdout(f\"test_conversation: {test_conversation} with id: {saved_conversation_id}\\nis of type{type(test_conversation)}\\n\\n\", stream=True)\n",
    "    \n",
    "    test_expectation = create_expectation(reasoning=mock_expectation_data[\"reasoning\"], user_predictions=mock_expectation_data[\"user_predictions\"],     additional_data=mock_expectation_data[\"additional_data\"])\n",
    "    saved_expectation_id = create_model(collection_name=\"expectations\", model_data=test_expectation, client=db_client)\n",
    "    chat_output.append_stdout(f\"test_expectation: {test_expectation} with id: {saved_expectation_id}\\nis of type{type(test_expectation)}\\n\\n\", stream=True)\n",
    "    \n",
    "    test_expectation_revision = create_expectation_revision(revised_input_possibilities=mock_expectation_revision_data[\"revised_input_possibilities\"],     prediction_error=-0.045, initial_expectation_id=saved_expectation_id)\n",
    "    saved_expectation_revision_id = create_model(collection_name=\"expectation_revisions\", model_data=test_expectation_revision, client=db_client)\n",
    "    chat_output.append_stdout(f\"test_expectation_revision: {test_expectation_revision} with id: {saved_expectation_revision_id}\\nis of type{type    (test_expectation_revision)}\\n\\n\", stream=True)\n",
    "    \n",
    "    test_message = create_chat_message(user_id=saved_user_id, content=mock_message_data[\"content\"], conversation_id=saved_conversation_id)\n",
    "    saved_message_id = create_model(collection_name=\"chat_messages\", model_data=test_message, client=db_client)\n",
    "    chat_output.append_stdout(f\"test_message: {test_message} with id: {saved_message_id}\\nis of type{type(saved_message_id)}\\n\\n\", stream=True)\n",
    "    \n",
    "    test_violation = create_violation(last_llm_response_id=None, expectation_id=saved_expectation_id, voe_thought=mock_violation_data[\"voe_thought\"])\n",
    "    saved_violation_id = create_model(collection_name=\"violations\", model_data=test_violation, client=db_client)\n",
    "    chat_output.append_stdout(f\"test_violation: {test_violation} with id: {saved_violation_id}\\nis of type{type(test_violation)}\\n\\n\", stream=True)\n",
    "    \n",
    "    test_chatbot_response = create_chatbot_response(content=mock_chatbot_response_data['content'], conversation_id=saved_conversation_id, response_to_id=saved_message_id)\n",
    "    saved_response_id = create_model(collection_name=\"chatbot_responses\", model_data=test_chatbot_response, client=db_client)\n",
    "    chat_output.append_stdout(f\"test_chatbot_response {test_chatbot_response} with id: {saved_response_id}\\nis of type{type(test_chatbot_response)}\")\n",
    "\n",
    "    get_model(collection_name=\"users\", model_id=saved_user_id, client=db_client)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "apikey = os.getenv(\"OPENAI_API_KEY\")\n",
    "openai_client = OpenAI(\n",
    "    api_key=apikey\n",
    ")\n",
    "openai.api_key=apikey\n",
    "print(openai_client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_thought(prompts: List[Dict[str, str]]) -> str:\n",
    "    try:\n",
    "        \n",
    "        thought_completion = openai_client.chat.completions.create(\n",
    "            messages=prompts,\n",
    "            model=\"gpt-3.5-turbo\"\n",
    "        )\n",
    "        print(thought_completion)\n",
    "        \n",
    "        thought = thought_completion.choices[0].message.content\n",
    "        \n",
    "        return str(thought)\n",
    "    except OpenAIError as e:\n",
    "        #Handle API error here, e.g. retry or log\n",
    "        print(f\"OpenAI API returned an API Error: {e}\")\n",
    "        raise e\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stream_chatbot() -> str:\n",
    "    try:\n",
    "        chat_completion_stream = openai_client.chat.completions.create(\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"user\" if item.get(\"user_id\") else \"assistant\",\n",
    "                    \"content\": item[\"content\"]\n",
    "                }\n",
    "                for item in chat_history\n",
    "            ],\n",
    "            model=\"gpt-3.5-turbo\",\n",
    "            stream=True\n",
    "        )\n",
    "        print(chat_completion_stream)\n",
    "        message = \"\"\n",
    "        chat_output.append_stdout(f\"Chatbot: \", stream=True)\n",
    "        for chunk in chat_completion_stream:\n",
    "            if chunk.choices[0].delta.content is not None:\n",
    "                chat_output.append_stdout(f\"{chunk.choices[0].delta.content}\", stream=True)\n",
    "                message += chunk.choices[0].delta.content\n",
    "        chat_output.append_stdout(f\"\\n\")\n",
    "        \n",
    "        return message\n",
    "    except OpenAIError as e:\n",
    "        #Handle API error here, e.g. retry or log\n",
    "        print(f\"OpenAI API returned an API Error: {e}\")\n",
    "        raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_user_message(user_message):\n",
    "    chat_output.append_stdout(f\"You: {user_message}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_button_click():\n",
    "    chat_output.append_stdout(f\"handle button clicked\", debug=True)\n",
    "    chat_output.append_stdout(f\"text_input.value: {text_input.value}\", debug=True)\n",
    "    user_input_queue.put(text_input.value)\n",
    "    text_input.value=''\n",
    "    chat_output.append_stdout(f\"handle button clicked after setting queue value\", debug=True)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_chat(user_id: ObjectId, conversation_id: Optional[ObjectId] = None):\n",
    "    global set_user, set_conversation, chat_history\n",
    "    set_user = get_model(collection_name=\"users\", model_id=saved_user_id, client=db_client)\n",
    "    if not conversation_id:\n",
    "        new_conversation = create_conversation(user_id=user_id)\n",
    "        conversation_id = create_model(collection_name=\"conversations\", model_data=new_conversation, client=db_client)\n",
    "    set_conversation = get_model(collection_name=\"conversations\", model_id=conversation_id, client=db_client)\n",
    "    \n",
    "    chat_history = get_chat_history(conversation_id=conversation_id, client=db_client)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_chat_history():\n",
    "    chat_output.append_stdout(f\"The chat history that belongs to this user and conversation:\")\n",
    "    for item in chat_history:\n",
    "        if item.get('user_id'):\n",
    "            print_user_message(item.get('content'))\n",
    "        else:\n",
    "            chat_output.append_stdout(f\"Chatbot: \", stream=True)\n",
    "            chat_output.append_stdout(item.get('content'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def user_prediction_task() -> dict:\n",
    "    # invoke reasoning process with Engagement Monitor Service\n",
    "    reasoning = mock_expectation_data[\"reasoning\"]\n",
    "    \n",
    "    # invoke user predictions with LLM Service\n",
    "    # user_predictions = mock_expectation_data[\"user_predictions\"]\n",
    "    expectation_prompt = build_expectation_prompt(first_message=True if len(chat_history) == 0 else False)\n",
    "    prompt = [\n",
    "        {\"role\": \"system\", \"content\": expectation_prompt}\n",
    "    ]\n",
    "    user_predictions = create_thought(prompts=prompt)\n",
    "    \n",
    "    \n",
    "    # invoke vector db fact fetching with Knowledge Assessment Service\n",
    "    additional_data = mock_expectation_data[\"additional_data\"]\n",
    "    \n",
    "    # create and save Expectation\n",
    "    new_expectation = create_expectation(reasoning=reasoning, user_predictions=user_predictions, additional_data=additional_data)\n",
    "    expectation_id = create_model(collection_name=\"expectations\", model_data=new_expectation, client=db_client)\n",
    "    \n",
    "    # invoke prediction improvement with LLM Service\n",
    "    # improved_predictions = mock_expectation_revision_data[\"revised_input_possibilities\"]\n",
    "    expectation_revision_prompt = build_expectation_revision_prompt(user_facts=additional_data)\n",
    "    prompt = [\n",
    "        {\"role\": \"system\", \"content\": expectation_revision_prompt}\n",
    "    ]\n",
    "    improved_predictions  = create_thought(prompts=prompt)\n",
    "    expectations_queue.put(improved_predictions)\n",
    "    # invoke prediction error calculation with ??? Service\n",
    "    prediction_error = 0.0\n",
    "    \n",
    "    # create and save ExpectationRevision\n",
    "    new_expectation_revision = create_expectation_revision(revised_input_possibilities=improved_predictions, prediction_error=prediction_error, initial_expectation_id=expectation_id)\n",
    "    expectation_revision_id = create_model(collection_name=\"expectation_revisions\", model_data=new_expectation_revision, client=db_client)\n",
    "    \n",
    "    expectation_revision_dict = get_model(collection_name=\"expectation_revisions\", model_id=expectation_revision_id, client=db_client)\n",
    "    \n",
    "    chat_output.append_stdout(f\"USER PREDICTION TASK COMPLETED\\nexpectation_revision_dict ID: {expectation_revision_dict['_id']}\", debug=True)\n",
    "    \n",
    "    return expectation_revision_dict\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def user_input_task() -> dict:\n",
    "    chat_output.append_stdout(f\"USER INPUT TASK\", debug=True)\n",
    "    chat_output.append_stdout(f\"user_input_queue: {user_input_queue.__dict__}\", debug=True)\n",
    "    \n",
    "    try:\n",
    "        user_input = user_input_queue.get()\n",
    "        \n",
    "        print_user_message(user_input)\n",
    "        \n",
    "        new_chat_message = create_chat_message(user_id=set_user['_id'], content=user_input, conversation_id=set_conversation['_id'])\n",
    "        chat_message_id = create_model(collection_name=\"chat_messages\", model_data=new_chat_message, client=db_client)\n",
    "        chat_message_dict = get_model(collection_name=\"chat_messages\", model_id=chat_message_id, client=db_client)\n",
    "        \n",
    "        chat_output.append_stdout(f\"USER INPUT TASK COMPLETED\\nchat message dict: {chat_message_dict}\", debug=True)\n",
    "        \n",
    "        chat_history.append(chat_message_dict)\n",
    "        \n",
    "        return chat_message_dict\n",
    "    \n",
    "    except Exception as e:\n",
    "        raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def voe_task(expectation_id: ObjectId) -> dict:\n",
    "    try:\n",
    "        # invoke violation of expectation with LLM service\n",
    "        # voe_thought = mock_violation_data['voe_thought']\n",
    "        last_expectation = expectations_queue.get()\n",
    "        last_llm_response = chat_history[len(chat_history)-2].get('content')\n",
    "        last_input = chat_history[len(chat_history)-1].get('content')\n",
    "\n",
    "        voe_prompt = build_voe_prompt(last_llm_response=last_llm_response, expectation=last_expectation, violation=last_input)\n",
    "        prompt = [\n",
    "            {\"role\": \"system\", \"content\": voe_prompt},\n",
    "        ]\n",
    "        voe_thought = create_thought(prompt)\n",
    "        \n",
    "        # implement DB call\n",
    "        # last_llm_response = None\n",
    "        \n",
    "        new_violation = create_violation(last_llm_response_id=last_llm_response, expectation_id=expectation_id, voe_thought=voe_thought)\n",
    "        violation_id = create_model(collection_name=\"violations\", model_data=new_violation, client=db_client)\n",
    "        violation_dict = get_model(collection_name=\"violations\", model_id=violation_id, client=db_client)\n",
    "        \n",
    "        return violation_dict\n",
    "    \n",
    "    except Exception as e:\n",
    "        raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chatbot_response_task(response_to_id: ObjectId):\n",
    "    try:\n",
    "        user_message_dict = get_model(collection_name=\"chat_messages\", model_id=response_to_id, client=db_client)\n",
    "        chat_output.append_stdout(f\"{user_message_dict}\", debug=True)\n",
    "        user_message_model = user_message_dict\n",
    "        chat_output.append_stdout(f\"user_message_model: {user_message_model}\",debug=True)\n",
    "        \n",
    "        # invoke prompt creation with useful information\n",
    "        \n",
    "        # invoke chatbot response with LLM Service\n",
    "        generated_response = stream_chatbot()\n",
    "        chat_output.append_stdout(f\"generated_response: {generated_response}\", debug=True)\n",
    "        \n",
    "        # save final API output as \"content\"\n",
    "        content = generated_response\n",
    "        \n",
    "        new_chatbot_response = create_chatbot_response(content=content, conversation_id=set_conversation['_id'], response_to_id=response_to_id)\n",
    "        response_id = create_model(collection_name=\"chatbot_responses\", model_data=new_chatbot_response, client=db_client)\n",
    "        response_dict = get_model(collection_name=\"chatbot_responses\", model_id=response_id, client=db_client)\n",
    "\n",
    "        chat_history.append(response_dict)\n",
    "        \n",
    "        return response_dict\n",
    "    \n",
    "    except Exception as e:\n",
    "        raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conversation_loop():\n",
    "    global text_input\n",
    "    active = True\n",
    "    text_input = widgets.Text(placeholder='Type your message here...')\n",
    "    submit_button = widgets.Button(description='Submit')\n",
    "    exit_button = widgets.Button(description='Exit')\n",
    "\n",
    "    def on_submit_button_clicked(b):\n",
    "        with chat_output:\n",
    "            handle_button_click()\n",
    "            if active:\n",
    "                # invoke user prediction task and return prediction object\n",
    "                expectation_dict = user_prediction_task()\n",
    "                chat_output.append_stdout(f\"Received prediction: {expectation_dict}\", debug=True)\n",
    "                expectation_id = expectation_dict[\"_id\"]\n",
    "\n",
    "                # invoke user input task and return user message\n",
    "                message_dict = user_input_task()\n",
    "                chat_output.append_stdout(f\"Received message_dict: {message_dict}\", debug=True)                \n",
    "                message_id = message_dict[\"_id\"]\n",
    "    \n",
    "                # invoke violation of expectation task and return violation object\n",
    "                violation_dict = voe_task(expectation_id=expectation_id)\n",
    "                chat_output.append_stdout(f\"Received violation: {violation_dict}\", debug=True)\n",
    "                \n",
    "                # invoke vector db fact storing with Knowledge Assessment Service\n",
    "                \n",
    "                # invoke chatbot response task and return chatbot message\n",
    "                chatbot_response_dict = chatbot_response_task(message_id)\n",
    "                chat_output.append_stdout(f\"Received response: {chatbot_response_dict}\", debug=True)\n",
    "\n",
    "    submit_button.on_click(on_submit_button_clicked)\n",
    "\n",
    "    def on_exit_button_clicked(b):\n",
    "        nonlocal active\n",
    "        with chat_output:\n",
    "            active = False\n",
    "            chat_output.clear_output()\n",
    "            chat_output.append_stdout(\"Exiting conversation loop...\")\n",
    "\n",
    "    exit_button.on_click(on_exit_button_clicked)\n",
    "\n",
    "    display(widgets.VBox([chat_output, text_input, submit_button, exit_button]))\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    test_items()\n",
    "    initialize_chat(saved_user_id, saved_conversation_id)\n",
    "    chat_output.append_stdout(f\"The user and conversation objects this conversation is set to:\")\n",
    "    chat_output.append_stdout(f\"user: {set_user}\\nconversation: {set_conversation}\")\n",
    "    print_chat_history()\n",
    "    conversation_loop()\n",
    "\n",
    "if __name__ == \"__main__\": main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
